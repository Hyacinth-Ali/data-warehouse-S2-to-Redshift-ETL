{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Data Warehouse (S3 to AWS Redshift ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Provision computing resources with Infrasctructure as Code paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Datawarehouse Params from a file\n",
    "Loads datawarehouse credentials that are required to provision the computing resource in AWS Redshift datawarehouse. Also, the credentials allows the project to interact with the databases in the Redshift clusters.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the configuration file so that the credntials values\n",
    "# can be extracted to initialize the corresponding variables\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "# Access Keys\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "# Cluster Details\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# Cluster Database Details\n",
    "HOST                   = config.get(\"CLUSTER\", \"HOST\")\n",
    "DB_NAME                = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER                = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD            = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT                = config.get(\"CLUSTER\",\"DB_PORT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clients for IAM, EC2, S3 and Redshift\n",
    "**Note** that these resources are created in the the **us-west-2** region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET,\n",
    "                       region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Create IAM ROLE\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::213424942515:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Redshift Cluster\n",
    "\n",
    "Create redshift cluster in **us-west-2** region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (ClusterAlreadyExists) when calling the CreateCluster operation: Cluster already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DB_NAME,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DB_USER,\n",
    "        MasterUserPassword=DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### *Describe* the cluster to see its status\n",
    "- This block od code should be run several times until the cluster status becomes `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClusterNotFoundFault",
     "evalue": "An error occurred (ClusterNotFound) when calling the DescribeClusters operation: Cluster dwhcluster not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClusterNotFoundFault\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     x \u001b[38;5;241m=\u001b[39m [(k, v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m props\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keysToShow]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mx, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m myClusterProps \u001b[38;5;241m=\u001b[39m \u001b[43mredshift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mClusterIdentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDWH_CLUSTER_IDENTIFIER\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClusters\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m prettyRedshiftProps(myClusterProps)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/client.py:401\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m py_operation_name)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/client.py:731\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    729\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    730\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClusterNotFoundFault\u001b[0m: An error occurred (ClusterNotFound) when calling the DescribeClusters operation: Cluster dwhcluster not found."
     ]
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Take note of the cluster **endpoint** and **role ARN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Ensure that the cluster status becomes \"Available\" before running this code. </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.cotzwka5s709.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::213424942515:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open an incoming  TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-000bcd5a66f7fb94f')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DB_PORT),\n",
    "        ToPort=int(DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate connection to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the redshift\n",
    "try: \n",
    "    connect = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(DWH_ENDPOINT, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT))\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not make connection to the cluster database\")\n",
    "    print(e)\n",
    "try: \n",
    "    cursor = connect.cursor()\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not get cursor to the Clsuter Database\")\n",
    "    print(e)\n",
    "connect.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tables Based on Star Schema approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# STEP 2: Create staging tables and then load dataset from S3 to the tables in redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Review the structure of the datasets from S3\n",
    "Since the staging tables are copies of the data sources, the structure of the data sources are reviewed before creating the staging tables.\n",
    "\n",
    "There are three datasets in S3 (Song, log, and log_json_path datasets). The log_json_path file contains the meta information that is required by AWS to correctly load lod datasets. The structure of the datasets are shown below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Dataset\n",
    "\n",
    "\n",
    "<img src=\"images/song_data.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Data\n",
    "\n",
    "<img src=\"images/log_data.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log_json_path Dataset\n",
    "\n",
    "<img src=\"images/log_json_path.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Create staging song events table\n",
    "The staging tables are copies of the datasets from S3. Hence, the structure of the staging table aligns with the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset staging tables to facilitate ETL pipeline test\n",
    "It is recommended to run these block of code after creating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset staging song events table\n",
    "staging_events_table_drop = \"DROP TABLE staging_events_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_events_table_drop)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue dropping table\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset staging song data table\n",
    "staging_songs_table_drop = \"DROP TABLE staging_songs_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_songs_table_drop)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue dropping table\")\n",
    "    print (e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create Dataset Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create staging song events table\n",
    "staging_events_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS staging_events_table \n",
    "        (\n",
    "            artist text, \n",
    "            auth text,\n",
    "            firstName text,\n",
    "            gender text,\n",
    "            itemInSession int4,\n",
    "            lastName text,\n",
    "            length float,\n",
    "            level text,\n",
    "            location text,\n",
    "            method text,\n",
    "            page text,\n",
    "            registration float,\n",
    "            sessionId int4,\n",
    "            song text,\n",
    "            status int4,\n",
    "            ts bigint,\n",
    "            userAgent text,\n",
    "            userId int\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_events_table_create)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue creating table\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create staging song dataset\n",
    "staging_songs_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS staging_songs_table\n",
    "    (\n",
    "      num_songs int4,\n",
    "      artist_id text,\n",
    "      artist_latitude float,\n",
    "      artist_longitude float,\n",
    "      artist_location text,\n",
    "      artist_name text,\n",
    "      song_id text,\n",
    "      title text,\n",
    "      duration float,\n",
    "      year int\n",
    "\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_songs_table_create)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue creating table\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Copy Datasets from S3 to Staging Staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    COPY staging_events_table FROM 's3://udacity-dend/log_data'\n",
      "    CREDENTIALS 'aws_iam_role=arn:aws:iam::213424942515:role/dwhRole'\n",
      "    REGION 'us-west-2'\n",
      "    json 's3://udacity-dend/log_json_path.json'\n",
      "    dateformat 'auto';\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#copy staging event datasets\n",
    "staging_events_copy = (\"\"\"\n",
    "    COPY staging_events_table FROM 's3://udacity-dend/log_data'\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    REGION 'us-west-2'\n",
    "    json 's3://udacity-dend/log_json_path.json'\n",
    "    dateformat 'auto';\n",
    "\"\"\").format(DWH_ROLE_ARN)\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_events_copy)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues copying data from S3\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query staging song events table to ensure that datasets were copied successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8056,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM staging_events_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    COPY staging_songs_table FROM 's3://udacity-dend/song_data'\n",
      "    CREDENTIALS 'aws_iam_role=arn:aws:iam::213424942515:role/dwhRole'\n",
      "    REGION 'us-west-2'\n",
      "    json 'auto';\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#copy staging song data datasets\n",
    "staging_songs_copy = (\"\"\"\n",
    "    COPY staging_songs_table FROM 's3://udacity-dend/song_data'\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    REGION 'us-west-2'\n",
    "    json 'auto';\n",
    "\"\"\").format(DWH_ROLE_ARN)\n",
    "\n",
    "print(staging_songs_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(staging_songs_copy)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues copying data from S3\")\n",
    "    print(e)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14896,)\n"
     ]
    }
   ],
   "source": [
    " query = (\"\"\"\n",
    "    SELECT COUNT(*) AS count FROM staging_songs_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Create Fact and Dimensional Tables That are Optimized for Data Analysis\n",
    "\n",
    "To analyse these datasets in redshift, I create dimensional tables from the staging tables based on a **star Schema** approach, as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/star_schema.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6.1: Create Tables - Star Schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dimensional Tables: users, songs, artist, and time tables\n",
    "These tables targets meta information about the domain concepts of the datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create User Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issues deleting a table\n",
      "Table \"user_table\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset users table to facilitate testing the ETL pipeline\n",
    "user_table_drop = \"DROP TABLE user_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(user_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User Table\n",
    "user_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_table\n",
    "    (\n",
    "        user_id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        first_name TEXT,\n",
    "        last_name TEXT,\n",
    "        gender TEXT,\n",
    "        level TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(user_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issues deleting a table\n",
      "Table \"song_table\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset songs table to facilitate testing the ETL pipeline\n",
    "song_table_drop = \"DROP TABLE song_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(song_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create song table\n",
    "song_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS song_table\n",
    "    (\n",
    "        song_id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        title TEXT,\n",
    "        artist_id TEXT,\n",
    "        year INT4,\n",
    "        duration float\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(song_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issues deleting a table\n",
      "Table \"artist_table\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset songs table to facilitate testing the ETL pipeline\n",
    "artist_table_drop = \"DROP TABLE artist_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(artist_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artist table\n",
    "artist_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS artist_table\n",
    "    (\n",
    "        artist_id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        location TEXT,\n",
    "        latitude float,\n",
    "        longitude float\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(artist_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issues deleting a table\n",
      "Table \"time_table\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset songs table to facilitate testing the ETL pipeline\n",
    "time_table_drop = \"DROP TABLE time_table CASCADE\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(time_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time table\n",
    "time_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS time_table\n",
    "    (\n",
    "        time_id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        start_time TIMESTAMP,\n",
    "        hour INT4,\n",
    "        day INT4,\n",
    "        week INT4,\n",
    "        month INT4,\n",
    "        year INT4,\n",
    "        weekday TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(time_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Fact Table - Song Play Table\n",
    "This table targets the log details of played songs, i.e., songs with page = `NextPage` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issues deleting a table\n",
      "Table \"songplay_table\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset datable to facilate testing the ETL pipeline\n",
    "songplay_table_drop = \"DROP TABLE songplay_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(songplay_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create song play query\n",
    "songplay_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS songplay_table\n",
    "    (\n",
    "        songplay_id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        start_time date REFERENCES time_table,\n",
    "        user_id INT4 REFERENCES user_table,\n",
    "        level TEXT,\n",
    "        song_id TEXT REFERENCES song_table,\n",
    "        artist_id TEXT REFERENCES artist_table,\n",
    "        sessionId int4,\n",
    "        location TEXT,\n",
    "        user_agent TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Create song play table\n",
    "try:\n",
    "    cursor.execute(songplay_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Datasets into Song Play Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_table_insert = (\"\"\"\n",
    "    INSERT INTO songplay_table (start_time, user_id, level, song_id, artist_id, sessionId, location, user_agent)\n",
    "    SELECT\n",
    "        to_timestamp(ts, 'YYYYMMDD HHMISS') AS start_time,\n",
    "        userId AS user_id,\n",
    "        level,\n",
    "        song_id,\n",
    "        artist_id,\n",
    "        sessionId,\n",
    "        location,\n",
    "        userAgent AS user_agent\n",
    "    FROM staging_events_table e\n",
    "    JOIN staging_songs_table s\n",
    "    ON (e.artist = s.artist_name)\n",
    "    AND (e.length = s.duration)\n",
    "    AND (e.song = s.title)\n",
    "    AND page = 'NextSong'\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(songplay_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the insertion of the datasets into song play table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM songplay_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "n_row = cursor.fetchone()\n",
    "print(n_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Insert Datasets to User Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table_insert = (\"\"\"\n",
    "    INSERT INTO user_table (first_name, last_name, gender, level)\n",
    "    SELECT DISTINCT\n",
    "        firstName AS first_name,\n",
    "        lastName AS last_name,\n",
    "        gender,\n",
    "        level\n",
    "    FROM staging_events_table;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(user_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Verify the insertion of the datasets into user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM user_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Insert Datasets into Songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_table_insert = (\"\"\"\n",
    "    INSERT INTO song_table (title, artist_id, year, duration)\n",
    "    SELECT DISTINCT\n",
    "        title,\n",
    "        artist_id,\n",
    "        year,\n",
    "        duration\n",
    "    FROM staging_songs_table;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(song_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Verify the insertion of the datasets into song table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14896,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM song_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Datasets into Artist Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table_insert = (\"\"\"\n",
    "    INSERT INTO artist_table (name, location, latitude, longitude)\n",
    "    SELECT DISTINCT\n",
    "        artist_name AS name,\n",
    "        artist_location AS location,\n",
    "        artist_latitude AS latitude,\n",
    "        artist_longitude AS longitude\n",
    "    FROM staging_songs_table;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(artist_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the insertion of the datasets into song table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM artist_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "nRows = cursor.fetchone()\n",
    "print(nRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Datasets into Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table_insert = (\"\"\"\n",
    "    INSERT INTO time_table (start_time, hour, day, week, month, year, weekday)\n",
    "    SELECT DISTINCT\n",
    "        start_time,\n",
    "        EXTRACT(hour FROM start_time) AS hour,\n",
    "        EXTRACT(day FROM start_time) AS day,\n",
    "        EXTRACT(week FROM start_time) AS week,\n",
    "        EXTRACT(month FROM start_time) AS month,\n",
    "        EXTRACT(year FROM start_time) AS year,\n",
    "        EXTRACT(weekday FROM start_time) AS weekday\n",
    "    FROM songplay_table;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(time_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the insertion of the datasets into time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) FROM time_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "count = cursor.fetchone()\n",
    "print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect.close()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Clean up your resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>DO NOT RUN THIS UNLESS YOU ARE SURE <br/> \n",
    "    We will be using these resources in the next exercises</span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'dwh_user',\n",
       "  'DBName': 'dwh',\n",
       "  'Endpoint': {'Address': 'dwhcluster.cotzwka5s709.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2023, 2, 10, 3, 33, 47, 28000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-000bcd5a66f7fb94f',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-0da6892cf67a5882d',\n",
       "  'AvailabilityZone': 'us-west-2c',\n",
       "  'PreferredMaintenanceWindow': 'wed:06:30-wed:07:00',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 4,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::213424942515:role/dwhRole',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2023, 2, 15, 6, 30, tzinfo=tzutc()),\n",
       "  'TotalStorageCapacityInMegaBytes': 1600000,\n",
       "  'AquaConfiguration': {'AquaStatus': 'disabled',\n",
       "   'AquaConfigurationStatus': 'auto'}},\n",
       " 'ResponseMetadata': {'RequestId': '6b205922-0ceb-4945-b00a-8b72e38c2cbc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6b205922-0ceb-4945-b00a-8b72e38c2cbc',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2713',\n",
       "   'date': 'Fri, 10 Feb 2023 05:28:49 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run this block several times until the cluster really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClusterNotFoundFault",
     "evalue": "An error occurred (ClusterNotFound) when calling the DescribeClusters operation: Cluster dwhcluster not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClusterNotFoundFault\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m myClusterProps \u001b[38;5;241m=\u001b[39m \u001b[43mredshift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mClusterIdentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDWH_CLUSTER_IDENTIFIER\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClusters\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m prettyRedshiftProps(myClusterProps)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/client.py:401\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m py_operation_name)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/client.py:731\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    729\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    730\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClusterNotFoundFault\u001b[0m: An error occurred (ClusterNotFound) when calling the DescribeClusters operation: Cluster dwhcluster not found."
     ]
    }
   ],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1c94f719-386d-4674-bc27-e2513f219979',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1c94f719-386d-4674-bc27-e2513f219979',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '200',\n",
       "   'date': 'Fri, 10 Feb 2023 05:29:02 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
