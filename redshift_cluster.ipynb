{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Data Warehouse (S3 to AWS Redshift ETL)\n",
    "There five major steps that will be followed to implementhis project:\n",
    "1. Provision computing resources\n",
    "2. Create database tables\n",
    "3. Insert data into the tables\n",
    "4. Run exploratory data analysis\n",
    "5. Tear down resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STEP 1: Provision computing resources with Infrasctructure as Code paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Datawarehouse Params from a file\n",
    "Loads datawarehouse credentials that are required to provision the computing resource in AWS Redshift datawarehouse. Also, the credentials allows the project to interact with the databases in the Redshift clusters.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the configuration file so that the credntials values\n",
    "# can be extracted to initialize the corresponding variables\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "# Access Keys\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "# Cluster Details\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# Cluster Database Details\n",
    "HOST                   = config.get(\"CLUSTER\", \"HOST\")\n",
    "DB_NAME                = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER                = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD            = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT                = config.get(\"CLUSTER\",\"DB_PORT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clients for IAM, EC2, S3 and Redshift\n",
    "**Note** that these resources are created in the the **us-west-2** region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET,\n",
    "                       region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Create IAM ROLE\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::213424942515:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Redshift Cluster\n",
    "\n",
    "Create redshift cluster in **us-west-2** region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DB_NAME,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DB_USER,\n",
    "        MasterUserPassword=DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### *Describe* the cluster to see its status\n",
    "- This block od code should be run several times until the cluster status becomes `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ky/1tw7jqvx1mq7yv1nr9nj5n4h0000gn/T/ipykernel_5218/1230814843.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwh_user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.cotzwka5s709.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-0da6892cf67a5882d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                   Value  \n",
       "0  dwhcluster                                                                             \n",
       "1  dc2.large                                                                              \n",
       "2  available                                                                              \n",
       "3  dwh_user                                                                               \n",
       "4  dwh                                                                                    \n",
       "5  {'Address': 'dwhcluster.cotzwka5s709.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-0da6892cf67a5882d                                                                  \n",
       "7  4                                                                                      "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Take note of the cluster **endpoint** and **role ARN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Ensure that the cluster status becomes \"Available\" before running this code. </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.cotzwka5s709.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::213424942515:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open an incoming  TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-000bcd5a66f7fb94f')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DB_PORT),\n",
    "        ToPort=int(DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate connection to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the redshift\n",
    "try: \n",
    "    connect = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(DWH_ENDPOINT, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT))\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not make connection to the cluster database\")\n",
    "    print(e)\n",
    "try: \n",
    "    cursor = connect.cursor()\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not get cursor to the Clsuter Database\")\n",
    "    print(e)\n",
    "connect.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STEP 2: Create Final Tables Based on Star Schema approach.\n",
    "To create the tables, staging tables are first created before the final tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Review the structure of the datasets from S3\n",
    "Since the staging tables are copies of the data sources, the structure of the data sources are reviewed before creating the staging tables.\n",
    "\n",
    "There are three datasets in S3 (Song, log, and log_json_path datasets). The log_json_path file contains the meta information that is required by AWS to correctly load lod datasets. The structure of the datasets are shown below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Dataset\n",
    "\n",
    "\n",
    "<img src=\"images/song_data.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Data\n",
    "\n",
    "<img src=\"images/log_data.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log_json_path Dataset\n",
    "\n",
    "<img src=\"images/log_json_path.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create staging tables\n",
    "The staging tables are copies of the datasets from S3. Hence, the structure of the staging table aligns with the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reset staging tables to facilitate ETL pipeline test\n",
    "It is recommended to run these block of code after creating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issue dropping table\n",
      "Table \"staging_events_table\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset staging song events table\n",
    "staging_events_table_drop = \"DROP TABLE staging_events_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_events_table_drop)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue dropping table\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issue dropping table\n",
      "Table \"staging_songs_table\" does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset staging song data table\n",
    "staging_songs_table_drop = \"DROP TABLE staging_songs_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_songs_table_drop)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue dropping table\")\n",
    "    print (e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Dataset Staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create staging song events table\n",
    "staging_events_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS staging_events_table \n",
    "        (\n",
    "            artist text, \n",
    "            auth text,\n",
    "            firstName text,\n",
    "            gender text,\n",
    "            itemInSession int4,\n",
    "            lastName text,\n",
    "            length float,\n",
    "            level text,\n",
    "            location text,\n",
    "            method text,\n",
    "            page text,\n",
    "            registration float,\n",
    "            sessionId int4,\n",
    "            song text,\n",
    "            status int4,\n",
    "            ts bigint,\n",
    "            userAgent text,\n",
    "            userId int\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_events_table_create)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue creating table\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create staging song dataset\n",
    "staging_songs_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS staging_songs_table\n",
    "    (\n",
    "      num_songs int4,\n",
    "      artist_id text,\n",
    "      artist_latitude float,\n",
    "      artist_longitude float,\n",
    "      artist_location text,\n",
    "      artist_name text,\n",
    "      song_id text,\n",
    "      title text,\n",
    "      duration float,\n",
    "      year int\n",
    "\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_songs_table_create)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Issue creating table\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Fact and Dimensional Tables That are Optimized for Data Analysis\n",
    "\n",
    "To analyse these datasets in redshift, I create dimensional tables from the staging tables based on a **star Schema** approach, as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/star_schema.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Create Dimensional Tables: users, songs, artist, and time tables\n",
    "These tables targets meta information about the domain concepts of the datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create User Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset users table to facilitate testing the ETL pipeline\n",
    "user_table_drop = \"DROP TABLE user_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(user_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User Table\n",
    "user_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_table\n",
    "    (\n",
    "        id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        user_id INT4,\n",
    "        first_name TEXT,\n",
    "        last_name TEXT,\n",
    "        gender TEXT,\n",
    "        level TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(user_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset songs table to facilitate testing the ETL pipeline\n",
    "song_table_drop = \"DROP TABLE song_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(song_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create song table\n",
    "song_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS song_table\n",
    "    (\n",
    "        id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        song_id TEXT,\n",
    "        title TEXT,\n",
    "        artist_id TEXT,\n",
    "        year INT4,\n",
    "        duration float\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(song_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset songs table to facilitate testing the ETL pipeline\n",
    "artist_table_drop = \"DROP TABLE artist_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(artist_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artist table\n",
    "artist_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS artist_table\n",
    "    (\n",
    "        id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        artist_id TEXT,\n",
    "        name TEXT,\n",
    "        location TEXT,\n",
    "        latitude float,\n",
    "        longitude float\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(artist_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset songs table to facilitate testing the ETL pipeline\n",
    "time_table_drop = \"DROP TABLE time_table CASCADE\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(time_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time table\n",
    "time_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS time_table\n",
    "    (\n",
    "        id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        time_id INT4,\n",
    "        start_time TIMESTAMP,\n",
    "        hour INT4,\n",
    "        day INT4,\n",
    "        week INT4,\n",
    "        month INT4,\n",
    "        year INT4,\n",
    "        weekday TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    cursor.execute(time_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Fact Table - Song Play Table\n",
    "This table targets the log details of played songs, i.e., songs with page = `NextPage` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Issues deleting a table\n",
      "SSL SYSCALL error: EOF detected\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset datable to facilate testing the ETL pipeline\n",
    "songplay_table_drop = \"DROP TABLE songplay_table\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(songplay_table_drop)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues deleting a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create song play query\n",
    "songplay_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS songplay_table\n",
    "    (\n",
    "        songplay_id INT4 IDENTITY(0, 1) NOT NULL PRIMARY KEY,\n",
    "        start_time date REFERENCES time_table,\n",
    "        user_id INT4 REFERENCES user_table,\n",
    "        level TEXT,\n",
    "        song_id TEXT REFERENCES song_table,\n",
    "        artist_id TEXT REFERENCES artist_table,\n",
    "        sessionId int4,\n",
    "        location TEXT,\n",
    "        user_agent TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Create song play table\n",
    "try:\n",
    "    cursor.execute(songplay_table_create)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Creating a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STEP 3: Insert Datasets into the Tables.\n",
    "Isert data into the tables by copying datasets from S3 to staging tables, and finally inserting datasets from the staging tables to the final tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Copy Datasets from S3 to Staging Staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy staging event datasets\n",
    "staging_events_copy = (\"\"\"\n",
    "    COPY staging_events_table FROM 's3://udacity-dend/log_data'\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    REGION 'us-west-2'\n",
    "    json 's3://udacity-dend/log_json_path.json'\n",
    "    dateformat 'auto';\n",
    "\"\"\").format(DWH_ROLE_ARN)\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_events_copy)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues copying data from S3\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy staging song datasets\n",
    "staging_songs_copy = (\"\"\"\n",
    "    COPY staging_songs_table FROM 's3://udacity-dend/song_data'\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    REGION 'us-west-2'\n",
    "    json 'auto';\n",
    "\"\"\").format(DWH_ROLE_ARN)\n",
    "\n",
    "try:\n",
    "    cursor.execute(staging_songs_copy)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues copying data from S3\")\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Insert Datasets into Song Play Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_table_insert = (\"\"\"\n",
    "    INSERT INTO songplay_table (start_time, user_id, level, song_id, artist_id, sessionId, location, user_agent)\n",
    "    SELECT\n",
    "        to_timestamp(ts, 'YYYYMMDD HHMISS') AS start_time,\n",
    "        userId AS user_id,\n",
    "        level,\n",
    "        song_id,\n",
    "        artist_id,\n",
    "        sessionId,\n",
    "        location,\n",
    "        userAgent AS user_agent\n",
    "    FROM staging_events_table e\n",
    "    JOIN staging_songs_table s\n",
    "    ON (e.artist = s.artist_name)\n",
    "    AND (e.length = s.duration)\n",
    "    AND (e.song = s.title)\n",
    "    AND page = 'NextSong'\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(songplay_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Insert Datasets to User Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table_insert = (\"\"\"\n",
    "    INSERT INTO user_table (user_id, first_name, last_name, gender, level)\n",
    "    SELECT DISTINCT\n",
    "        userId as user_id,\n",
    "        firstName AS first_name,\n",
    "        lastName AS last_name,\n",
    "        gender,\n",
    "        level\n",
    "    FROM staging_events_table;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(user_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Insert Datasets into Songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_table_insert = (\"\"\"\n",
    "    INSERT INTO song_table (song_id, title, artist_id, year, duration)\n",
    "    SELECT DISTINCT\n",
    "        song_id,\n",
    "        title,\n",
    "        artist_id,\n",
    "        year,\n",
    "        duration\n",
    "    FROM staging_songs_table;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(song_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Insert Datasets into Artist Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table_insert = (\"\"\"\n",
    "    INSERT INTO artist_table (artist_id, name, location, latitude, longitude)\n",
    "    SELECT DISTINCT\n",
    "        artist_id,\n",
    "        artist_name AS name,\n",
    "        artist_location AS location,\n",
    "        artist_latitude AS latitude,\n",
    "        artist_longitude AS longitude\n",
    "    FROM staging_songs_table s\n",
    "    JOIN staging_events_table e\n",
    "    ON (e.artist = s.artist_name)\n",
    "    AND (e.length = s.duration)\n",
    "    AND (e.song = s.title)\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(artist_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Datasets into Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table_insert = (\"\"\"\n",
    "    INSERT INTO time_table (start_time, hour, day, week, month, year, weekday)\n",
    "    SELECT DISTINCT\n",
    "        start_time,\n",
    "        EXTRACT(hour FROM start_time) AS hour,\n",
    "        EXTRACT(day FROM start_time) AS day,\n",
    "        EXTRACT(week FROM start_time) AS week,\n",
    "        EXTRACT(month FROM start_time) AS month,\n",
    "        EXTRACT(year FROM start_time) AS year,\n",
    "        EXTRACT(weekday FROM start_time) AS weekday\n",
    "    FROM songplay_table;\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(time_table_insert)\n",
    "except Exception as e:\n",
    "    print(\"Error: Issues Inserting data to a table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STEP 4: Run Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rows in the staging event table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8056,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM staging_events_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rows in the staging songs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14896,)\n"
     ]
    }
   ],
   "source": [
    " query = (\"\"\"\n",
    "    SELECT COUNT(*) AS count FROM staging_songs_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rows in the staging songs played table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM songplay_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Number of rows in the user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM user_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Number of rows in the song table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14896,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM song_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rows in the artists table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "    SELECT COUNT(*) AS rows FROM artist_table\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "nRows = cursor.fetchone()\n",
    "print(nRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rows in the time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319,)\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "        SELECT COUNT(*) FROM time_table\n",
    "    \"\"\")\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "    connect.commit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "n_row = cursor.fetchone()\n",
    "print(n_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8056,)\n"
     ]
    }
   ],
   "source": [
    "connect.close()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STEP 5: Tear down resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run this block several times until the cluster really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
